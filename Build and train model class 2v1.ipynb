{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton Code\n",
    "\n",
    "The code below provides a skeleton for the model building & training component of your project. You can add/remove/build on code however you see fit, this is meant as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##Import any other stats/DL/ML packages you may need here. E.g. Keras, scikit-learn, etc.\n",
    "#Keras (https://keras.io/api/metrics/)\n",
    "#from tensorflow.keras import layers\n",
    "#scikit-learn (https://scikit-learn.org/stable/)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Do some early processing of your metadata for easier model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. add new column with the path to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans found: 112120 , Total Headers 112120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Finding Labels</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26211</th>\n",
       "      <td>00006875_030.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>30</td>\n",
       "      <td>6875</td>\n",
       "      <td>58</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2562</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_004/images/00006875_030.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>00026833_000.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>0</td>\n",
       "      <td>26833</td>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_011/images/00026833_000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>00000265_005.png</td>\n",
       "      <td>No Finding</td>\n",
       "      <td>5</td>\n",
       "      <td>265</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/data/images_001/images/00000265_005.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Image Index Finding Labels  Follow-up #  Patient ID  Patient Age  \\\n",
       "26211   00006875_030.png     No Finding           30        6875           58   \n",
       "101075  00026833_000.png     No Finding            0       26833           47   \n",
       "1087    00000265_005.png     No Finding            5         265           60   \n",
       "\n",
       "       Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
       "26211               F            PA                 2562     2991   \n",
       "101075              F            PA                 2992     2991   \n",
       "1087                M            PA                 2992     2991   \n",
       "\n",
       "        OriginalImagePixelSpacing[x     y]  Unnamed: 11  \\\n",
       "26211                         0.143  0.143          NaN   \n",
       "101075                        0.143  0.143          NaN   \n",
       "1087                          0.143  0.143          NaN   \n",
       "\n",
       "                                            path  \n",
       "26211   /data/images_004/images/00006875_030.png  \n",
       "101075  /data/images_011/images/00026833_000.png  \n",
       "1087    /data/images_001/images/00000265_005.png  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\n",
    "## Load the NIH data to all_xray_df\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Add coumn for each decease from the 'Finding Labels' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here you may want to create some extra columns in your table with binary indicators of certain diseases\n",
    "## rather than working directly with the 'Finding Labels' column\n",
    "##\n",
    "finding_labels = set('|'.join(all_xray_df['Finding Labels']).split('|'))\n",
    "\n",
    "for label in finding_labels:\n",
    "  all_xray_df[label] = all_xray_df['Finding Labels'].apply(lambda x: 1 if label in x else 0)\n",
    "#drop Findings label once decease has been split in different columns\n",
    "all_xray_df = all_xray_df.drop(['Finding Labels'], axis=1)\n",
    "#drop usless columm \n",
    "all_xray_df = all_xray_df.drop(['Unnamed: 11'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Image Index', 'Follow-up #', 'Patient ID', 'Patient Age',\n",
       "       'Patient Gender', 'View Position', 'OriginalImage[Width', 'Height]',\n",
       "       'OriginalImagePixelSpacing[x', 'y]', 'path', 'Fibrosis', 'Mass',\n",
       "       'Nodule', 'Emphysema', 'Consolidation', 'Pneumonia', 'Cardiomegaly',\n",
       "       'Edema', 'No Finding', 'Effusion', 'Pneumothorax', 'Atelectasis',\n",
       "       'Hernia', 'Pleural_Thickening', 'Infiltration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_headers = all_xray_df.columns\n",
    "column_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3. Add a column with the number of decease per record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "decease_columns = finding_labels.copy()\n",
    "decease_columns.remove('No Finding')\n",
    "all_xray_df['decease_number']=all_xray_df[decease_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate wrong input (age not possible) detected within the EDA phase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df = all_xray_df[all_xray_df['Patient Age']<120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Case data splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(all_xray_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Get all normal cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodecease_df = all_xray_df[all_xray_df['No Finding']==1]\n",
    "nodecease_data_size = len( nodecease_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Get deceased cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decease_df = all_xray_df[all_xray_df['No Finding']==0]\n",
    "decease_data_size = len(decease_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Get other decease cases without pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decease_wo_pneumonia_df = decease_df[decease_df.Pneumonia==0]\n",
    "decease_wo_pneumonia_data_size = len(decease_wo_pneumonia_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Get pneumonia cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decease_pneumonia_df= decease_df[decease_df.Pneumonia==1]\n",
    "decease_pneumonia_data_size = len(decease_pneumonia_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1. Get pure pneumonial cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_pure_df= decease_pneumonia_df[decease_pneumonia_df.decease_number<2]\n",
    "pneumonia_pure_data_size = len(pneumonia_pure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2. Get pneumonia with other decease case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pneumonia_other_df = decease_pneumonia_df[decease_pneumonia_df.decease_number>1]\n",
    "pneumonia_other_data_size = len(pneumonia_other_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy cases                 : 60353\n",
      "Decease cases                 : 51751\n",
      "  Not pneumonia cases         :   50321\n",
      "  Pneumonia cases             :   1430\n",
      "    -Pure cases               :     322\n",
      "    -With other deceases cases:     1108\n",
      "Total cases                   : 112104\n"
     ]
    }
   ],
   "source": [
    "print(f'Healthy cases                 : {nodecease_data_size}')\n",
    "print(f'Decease cases                 : {decease_data_size}')\n",
    "print(f'  Not pneumonia cases         :   {decease_wo_pneumonia_data_size}') \n",
    "print(f'  Pneumonia cases             :   {decease_pneumonia_data_size}')\n",
    "print(f'    -Pure cases               :     {pneumonia_pure_data_size}')\n",
    "print(f'    -With other deceases cases:     {pneumonia_other_data_size}')\n",
    "print(f'Total cases                   : {data_size}' )      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create your training and testing data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have 3 categories:\n",
    "\n",
    " - Negative (Healthy or other diseases) = 0\n",
    " - Pneumonia (with/without other deseases) = 1\n",
    "\n",
    "Because of the relatively few case of pneumonia case (1430) , we will augment using **geometrical** transformation using Keras data generator to some extends to have at least a ratio of 1/10 of the healthy cases. Since we don't have acces to radiologist to label image and we were not successful to extract distinct patterns when analysing image pixels , we won't use custom augmmentation by blurring or somoothing images.  \n",
    "\n",
    "On the other side we will tray to reduce by balancing not pneumonial case cross gender and age.\n",
    "\n",
    "Once, this augmentation done , we will initializing a convolutional neural network (CNN) with class weights calculate with the following formulas:\n",
    "\n",
    "weight_class_x = (1 / COUNT_CLASS_X) * (TOTAL_SAMPLES) / n\n",
    "\n",
    "n= number of class\n",
    "x = [1....n]\n",
    "\n",
    "Note : due to technical no resolved issues in our environment, unfortunately we can't run all the generation in the same session at onces without the kernel beeing interupted will be interrupted  , we 'll generate each category separrately within files to prepare the training and testing phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodecease_df=nodecease_df.drop(columns=['category'])\n",
    "#decease_wo_pneumonia_df=decease_wo_pneumonia_df.drop(columns=['category'])\n",
    "#pneumonia_pure_df=pneumonia_pure_df.drop(columns=['category'])\n",
    "#pneumonia_other_df=pneumonia_other_df.drop(columns=['category'])\n",
    "#nodecease_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['NO_PNEUMONIA','PNEUMONIA'] \n",
    "nodecease_df=nodecease_df.copy()\n",
    "nodecease_df.loc[:,'category']=categories[0] #'NO_DECEASE'\n",
    "decease_wo_pneumonia_df= decease_wo_pneumonia_df.copy()\n",
    "decease_wo_pneumonia_df.loc[:,'category']=categories[0] #'DECEASE_WO_PNEUMONIA'\n",
    "\n",
    "\n",
    "pneumonia_pure_df = pneumonia_pure_df.copy()\n",
    "pneumonia_pure_df.loc[:,'category']=categories[1]\n",
    "pneumonia_other_df = pneumonia_other_df.copy()\n",
    "pneumonia_other_df.loc[:,'category']=categories[1]\n",
    "pneumonia_all_df= pd.concat([pneumonia_pure_df,pneumonia_other_df],ignore_index=True)\n",
    "#shuffle\n",
    "pneumonia_all_df = pneumonia_all_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#not rally nice code , but we have to merge the pneumonia case for technical issue when trainig \n",
    "#w_p_o = len(pneumonia_other_df)\n",
    "#w_p_p = len(pneumonia_pure_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.0. Helper's Functions and infrastructure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to append file paths to a specific category and type of data in the dictionary\n",
    "\n",
    "'''\n",
    "def append_to_paths_dictonary(paths_dic, category, data_type, path):\n",
    "    if category in paths_dic:\n",
    "        if data_type in paths_dic[category]:\n",
    "            print(f'append - {category}.{data_type}')\n",
    "            paths_dic[category][data_type].append(path)\n",
    "        else:\n",
    "            print(f'new - {category}.{data_type}')\n",
    "            paths_dic[category][data_type] = [path]\n",
    "    else:\n",
    "        paths_dic[category] = {data_type: [path]}\n",
    "        \n",
    "'''\n",
    "Function to access the filepath in the paths_dict with the category and type of data as parameter\n",
    "'''\n",
    "def get_from_paths_dictonary(paths_dic,category,data_type):\n",
    "    if category in paths_dic and data_type in paths_dic[category]:\n",
    "        return paths_dic[category][data_type]\n",
    "    else:\n",
    "        return None\n",
    "'''\n",
    "Function to get all file paths from the dictionary\n",
    "'''    \n",
    "def get_all_paths_from_paths_dictonary(paths_dic):\n",
    "    all_paths = []\n",
    "    for category, types in paths_dic.items():\n",
    "        for file_type, paths in types.items():\n",
    "            all_paths.extend(paths)\n",
    "    return all_paths    \n",
    "\n",
    "def remove_directory(path):\n",
    "    \"\"\"\n",
    "    Removes a directory and its contents.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the directory to be removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Directory '{path}' and its contents removed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing directory '{path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new - NO_PNEUMONIA.train\n",
      "new - NO_PNEUMONIA.val\n",
      "new - PNEUMONIA.train\n",
      "new - PNEUMONIA.val\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/test' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/train' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/val' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/test' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/train' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/val' and its contents removed successfully.\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/test' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/train' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/NO_PNEUMONIA/val' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/test' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/train' and its contents created successfully.\n",
      "Directory '/workspace/home/generated/PNEUMONIA/val' and its contents created successfully.\n"
     ]
    }
   ],
   "source": [
    "## Initialization procedure\n",
    "import os\n",
    "\n",
    "generated_root_dir = '/workspace/home/generated'\n",
    "\n",
    "# Create the main directory\n",
    "os.makedirs(generated_root_dir, exist_ok=True)\n",
    "\n",
    "# Initialize an empty directory for generated paths for each category and type of data\n",
    "generated_file_paths_dic = {}\n",
    "\n",
    "#generate filepaths directory names\n",
    "for category in categories:\n",
    "    test_path = f'{generated_root_dir}/{category}/test'\n",
    "    append_to_paths_dictonary(generated_file_paths_dic,category,'test',test_path)\n",
    "    train_path = f'{generated_root_dir}/{category}/train'\n",
    "    append_to_paths_dictonary(generated_file_paths_dic,category,'train',train_path)\n",
    "    val_path = f'{generated_root_dir}/{category}/val'\n",
    "    append_to_paths_dictonary(generated_file_paths_dic,category,'val',val_path)\n",
    "\n",
    "    \n",
    "    \n",
    "paths_to_create = get_all_paths_from_paths_dictonary(generated_file_paths_dic)\n",
    "\n",
    "# clean\n",
    "for path  in paths_to_create:\n",
    "    remove_directory(path)\n",
    "\n",
    "# Create subdirectories\n",
    "for path  in paths_to_create:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"Directory '{path}' and its contents created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home\n"
     ]
    }
   ],
   "source": [
    "#cleaning procedure\n",
    "#cd /workspace/home/generated\n",
    "%cd /workspace/home\n",
    "\n",
    "#!rm r test\n",
    "#!rm r train\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. Function for data augmentation generators definitions and mix of generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in datagen.flow(x, batch_size=1, save_to_dir='path/to/save/augmented', save_prefix='aug', save_format='jpeg'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default no transformation\n",
    "\n",
    "\n",
    "def make_generator(df,\n",
    "                   directory,\n",
    "                   rescale,\n",
    "                   h_flip,\n",
    "                   h_shift_range,\n",
    "                   w_shift_range,\n",
    "                   rotation_range,\n",
    "                   shear_range,\n",
    "                   zoom_range,\n",
    "                   x_col,\n",
    "                   y_col,\n",
    "                   target_size,\n",
    "                   batch_size,\n",
    "                   classes,\n",
    "                   class_mode = 'sparse'):\n",
    "    print(f'batch size : {batch_size}')\n",
    "    idg = ImageDataGenerator(rescale=rescale,\n",
    "                             horizontal_flip = h_flip,\n",
    "                             height_shift_range = h_shift_range,\n",
    "                             width_shift_range = w_shift_range,\n",
    "                             rotation_range = rotation_range,\n",
    "                             shear_range = shear_range,\n",
    "                             zoom_range = zoom_range)   \n",
    "    print(f'{directory}')\n",
    "    gen =idg.flow_from_dataframe(dataframe=df,\n",
    "                                 directory=directory,\n",
    "                                 x_col= x_col,\n",
    "                                 y_col = y_col,\n",
    "                                 classes=classes,\n",
    "                                 class_mode = class_mode,\n",
    "                                 target_size = target_size,\n",
    "                                 batch_size=batch_size)\n",
    "\n",
    "    return gen\n",
    "\n",
    "def generate_images(df,\n",
    "                   train_directory,\n",
    "                   val_directory,\n",
    "                   test_directory,\n",
    "                   rescale, \n",
    "                   h_flip,\n",
    "                   h_shift_range,\n",
    "                   w_shift_range,\n",
    "                   rotation_range,\n",
    "                   shear_range,\n",
    "                   zoom_range,\n",
    "                   x_col,\n",
    "                   y_col,\n",
    "                   classes, \n",
    "                   factor, \n",
    "                   tgt_size, \n",
    "                   batch_size, \n",
    "                   ratio=0.8, \n",
    "                   seed =42): \n",
    "    '''\n",
    "    Generate training adn test image generator\n",
    "    Parameters:\n",
    "\n",
    "    Return:\n",
    "        train_generator,train_size,test_generator,test_size\n",
    "    '''\n",
    "    print(f'factor : {factor}')\n",
    "\n",
    "    train_df = df.sample(frac=ratio,random_state = seed)\n",
    "    left_df = df.drop(train_df.index).copy()\n",
    "    val_df  = left_df.sample(frac=0.5,random_state = seed)\n",
    "    test_df = left_df.drop(val_df.index)\n",
    "    \n",
    "    train_size = len(train_df)*factor//batch_size\n",
    "    print(f'train size = {train_size} = {len(train_df)}*{factor}//{batch_size}')\n",
    "                          \n",
    "    #no factor apply for test and val \n",
    "    #val_size   = len(val_df)*factor\n",
    "    val_size   = len(val_df)//batch_size\n",
    "    #test_size = len(test_df)* factor\n",
    "    test_size = len(test_df)//batch_size\n",
    "\n",
    "    #val_batch_size = val_size//batch_size\n",
    "    #val_test_size = test_size//batch_size\n",
    "    #print(f'val_batch_size: {val_batch_size}')\n",
    "    #print(f'val_test_size: {val_test_size}')\n",
    "\n",
    "    if train_directory:\n",
    "        train_generator = make_generator(df=train_df,\n",
    "                                     directory=train_directory, \n",
    "                                     rescale=rescale,\n",
    "                                     h_flip=h_flip,\n",
    "                                     h_shift_range=h_shift_range,\n",
    "                                     w_shift_range=w_shift_range,\n",
    "                                     rotation_range=rotation_range,\n",
    "                                     shear_range=shear_range,\n",
    "                                     zoom_range=zoom_range,\n",
    "                                     x_col=x_col,\n",
    "                                     y_col=y_col,\n",
    "                                     classes=classes,\n",
    "                                     target_size = tgt_size, \n",
    "                                     batch_size = batch_size)\n",
    "            \n",
    "    if val_directory:\n",
    "        val_generator = make_generator(df=val_df,\n",
    "                                     directory=val_directory, \n",
    "                                     rescale=rescale,\n",
    "                                     h_flip=False,\n",
    "                                     h_shift_range=0,\n",
    "                                     w_shift_range=0,\n",
    "                                     rotation_range=0,\n",
    "                                     shear_range=0,\n",
    "                                     zoom_range=0,\n",
    "                                     x_col=x_col,\n",
    "                                     y_col=y_col,\n",
    "                                     classes=classes,\n",
    "                                     target_size = tgt_size, \n",
    "                                     batch_size = batch_size)\n",
    "    if test_directory:\n",
    "        test_generator = make_generator(df=test_df,\n",
    "                                    directory=test_directory,\n",
    "                                    rescale=rescale,\n",
    "                                    h_flip=False,\n",
    "                                    h_shift_range=0,\n",
    "                                    w_shift_range=0,\n",
    "                                    rotation_range=0,\n",
    "                                    shear_range=0,\n",
    "                                    zoom_range=0,\n",
    "                                    x_col=x_col,\n",
    "                                    y_col=y_col,\n",
    "                                    classes=classes,\n",
    "                                    target_size = tgt_size, \n",
    "                                    batch_size = batch_size )                  \n",
    "    \n",
    "    \n",
    "    return train_generator,train_size,val_generator,val_size,test_generator,test_size\n",
    "    '''\n",
    "    keep code for usage\n",
    "    print(f'Generate {train_size} train sample')\n",
    "    while len(train_Xs)< train_size:\n",
    "        X,Y =  train_generator.next()\n",
    "        b_size = X.shape[0]\n",
    "        for i in range(b_size):\n",
    "            train_Xs.append(X[i,:,:,0])\n",
    "            train_Ys.append(Y[i])\n",
    "                      \n",
    "    print(f'Generate {test_size} test sample')\n",
    "    while len(test_Xs)< test_size:\n",
    "        X,Y = test_generator.next()\n",
    "        b_size = X.shape[0]\n",
    "        for i in range(b_size):    \n",
    "            test_Xs.append(X[i,:,:,0])\n",
    "            #test_Ys.append(Y[i,:]) for generacity voi class_mode \n",
    "            test_Ys.append(Y[i])\n",
    "    return train_Xs, train_Ys,test_Xs,test_Ys\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import threading\n",
    "\n",
    "\n",
    "\n",
    "class MixedImageDataGenerator(object):\n",
    "    \"\"\"\n",
    "    Wrapper class that mixes data from multiple ImageDataGenerator instances.\n",
    "\n",
    "    Args:\n",
    "        generators (list): List of ImageDataGenerator objects.\n",
    "        limits (list): List of integers representing the number of batches\n",
    "                        to be generated from each generator before switching.\n",
    "        generator_type: type of generato 0 = train, 1 = validate , 2 = test                \n",
    "\n",
    "    Attributes:\n",
    "        generators (list): List of ImageDataGenerator objects.\n",
    "        limits (list): List of integers representing batch limits.\n",
    "        current_generator_index (int): Index of the currently active generator.\n",
    "        remaining_batches (list): List of integers representing remaining\n",
    "                                  batches per generator.\n",
    "        data_queue (deque): Queue to store generated batches (optional for efficiency).\n",
    "    \"\"\"\n",
    "    TYPE = ['train','validate','test']\n",
    "    TRAIN =  0\n",
    "    VALIDATE = 1\n",
    "    TEST = 2\n",
    "    \n",
    "    def __init__(self, generators, limits,generator_type,debug=False):\n",
    "        self.lock = threading.Lock()\n",
    "        self.generators = generators\n",
    "        self.limits = limits\n",
    "        self.current_generator_index = 0\n",
    "        self.remaining_batches = limits.copy()  # Directly assign the limits list\n",
    "        self.debug=debug\n",
    "        if self.debug:\n",
    "            print(f'Create Generator :{MixedImageDataGenerator.TYPE[generator_type]}')\n",
    "        self.generator_type = generator_type\n",
    "    def get_step_number(self):\n",
    "        return sum(self.limits)\n",
    "    def get_remaining_batches(self):\n",
    "        return sum(self.remaining_batches)\n",
    "    \n",
    "    def getType(self):\n",
    "        return self.generator_type\n",
    "    \n",
    "    def get_generator_number(self):\n",
    "        return(len(self.generators))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Makes the MixedImageDataGenerator object iterable,\n",
    "        allowing it to be used in a for loop.\n",
    "\n",
    "        Returns:\n",
    "            self: The MixedImageDataGenerator object itself.\n",
    "        \"\"\" \n",
    "        if  self.get_remaining_batches() == 0:\n",
    "            return self.repeat()\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Returns the next batch of data from the mixed generators.\n",
    "\n",
    "        Raises:\n",
    "            StopIteration: If all limits have been exhausted.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]: next called [pre lock]')\n",
    "        with self.lock:\n",
    "            #if self.debug:\n",
    "            print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]: next call ...')\n",
    "            while True:\n",
    "                #print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]Remaining batch:  {self.remaining_batches} limits: {self.limits}')\n",
    "                if sum(self.remaining_batches) <= 0:\n",
    "                    #if self.debug:\n",
    "                    print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}] all limits exhausted')\n",
    "                    raise StopIteration\n",
    "                \n",
    "                data = None\n",
    "                while data is None:\n",
    "                    # Check if current generator has remaining batches\n",
    "                    if self.remaining_batches[self.current_generator_index] > 0:\n",
    "                        # Ensure the current generator is advanced if its queue is empty\n",
    "                        data = self.generators[self.current_generator_index].next() \n",
    "                        # Decrease the remaining batches of the current generator\n",
    "                        self.remaining_batches[self.current_generator_index] -= 1\n",
    "                        if self.debug:\n",
    "                            print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]Remaining batch:  {self.remaining_batches} limits: {self.limits}')\n",
    "\n",
    "                        \n",
    "                    # Move to the next generator\n",
    "                    self.current_generator_index = (self.current_generator_index + 1) % len(self.generators)\n",
    "                    # If all generators have exhausted their limits, raise StopIteration\n",
    "                    if all(remaining_batch <= 0 for remaining_batch in self.remaining_batches):\n",
    "                        if self.debug:\n",
    "                            print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}] all step data generated')\n",
    "                        raise StopIteration\n",
    "                if self.debug:\n",
    "                    print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]: RETURN {type(data)}')\n",
    "                print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}]: next called')\n",
    "                return data\n",
    "\n",
    "                \n",
    "    def refresh(self, argument=None):      \n",
    "        with self.lock:\n",
    "            if self.debug:\n",
    "                print(f'[{MixedImageDataGenerator.TYPE[self.generator_type]}] refresh')\n",
    "            self.current_generator_index = 0\n",
    "            self.remaining_batches = self.limits.copy()  # Directly assign the limits list\n",
    "            # self.data_queue = deque(maxlen=10)  # Optional queue for efficiency\n",
    "\n",
    "    def repeat(self):\n",
    "        print('repeat called')\n",
    "        while True:\n",
    "            try:\n",
    "                yield next(self)\n",
    "            except StopIteration:\n",
    "                self.refresh()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class MixedImageDataGeneratorRefresherCallBack(Callback):\n",
    "    '''\n",
    "    Call back function to renitialize the generators for each epochs call internaly by fit_generator function\n",
    "    \n",
    "    Usage \n",
    "    \n",
    "    history = my_model.fit_generator(train_gen, steps_per_epoch=total_batches, epochs=num_epochs, callbacks=[callback])    \n",
    "    '''\n",
    "    def __init__(self, generators):\n",
    "        self.generators = generators\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(f'[ON_TRAIN_BEGIN]')\n",
    "\n",
    "        # first the refresh for because of pretest done within fit method\n",
    "        #for i in range(len(self.generators)):\n",
    "        #    print(f'[ON_TRAIN_BEGIN] refresh generator {i}')\n",
    "        #    self.generators[i].refresh()\n",
    "    def on_train_end(self, logs=None):\n",
    "        print(f'[ON_TRAIN_END]')\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        print(f'[ON_BATCH_BEGIN] batch {batch}')\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        print(f'[ON_BATCH_END] batch {batch}')\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f'[ON_EPOCH_BEGIN] epoch {epoch}')\n",
    "        #refresh generators before epochs\n",
    "        for i in range(len(self.generators)):\n",
    "            #if self.generators[i].getType() ==MixedImageDataGenerator.TRAIN:\n",
    "            print(f'[ON_EPOCH_BEGIN] refresh generator {i} epoch {epoch}')\n",
    "            self.generators[i].refresh()\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f'[ON_EPOCH_END] epoch {epoch}')\n",
    "\n",
    "\n",
    "    def on_test_begin(self,logs=None):\n",
    "        print(f'[ON_TEST_BEGIN]')\n",
    "        #for i in range(len(self.generators)):\n",
    "            #if self.generators[i].getType() ==MixedImageDataGenerator.VALIDATE:\n",
    "            #    print(f'[ON_TEST_BEGIN] refresh generator {i} - {MixedImageDataGenerator.VALIDATE}')\n",
    "            #    self.generators[i].refresh()    \n",
    "    \n",
    "    def on_test_end(self,logs=None):\n",
    "        print(f'[ON_TEST_END]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2. Augmententation of minority data by factor 10  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.0 Parameters intialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale= 1/255\n",
    "h_flip=True\n",
    "h_shift_range=0.1\n",
    "w_shift_range=0.1\n",
    "rotation_range=5\n",
    "shear_range=0\n",
    "zoom_range=0\n",
    "\n",
    "x_col = 'path'\n",
    "y_col = 'category'\n",
    "factor = 10\n",
    "#factor = 10\n",
    "\n",
    "## for VGG16\n",
    "#classes= ['NO_DECEASE','DECEASE_WO_PNEUMONIA','PNEUMONIA_PURE_DECEASE','PNEUMONIA_OTHER_DECEASE']\n",
    "#tgt_size = (224,224)\n",
    "tgt_size = (299,299)\n",
    "#batch_size = 1\n",
    "#batch_size = 2*20\n",
    "batch_size = 2*20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.1 Augmentation pure pneumonia cases generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrain_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'train')[0]\\nval_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'val')[0]\\ntest_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'test')[0]\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#categories = ['NO_DECEASE','DECEASE_WO_PNEUMONIA','PNEUMONIA_PURE_DECEASE','PNEUMONIA_OTHER_DECEASE'] \n",
    "#phase removed we merge all the case to avoid trainig interuption \n",
    "'''\n",
    "train_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'train')[0]\n",
    "val_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'val')[0]\n",
    "test_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[4],'test')[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngen_p_p_train,gen_p_p_train_size,gen_p_p_val,gen_p_p_val_size,gen_p_p_test,gen_p_p_test_size = generate_images(df=pneumonia_all_df,\\n                                                       train_directory= train_directory,\\n                                                       val_directory = val_directory,\\n                                                       test_directory = test_directory,\\n                                                              rescale=rescale,\\n                                                              h_flip=h_flip,\\n                                                              h_shift_range = h_shift_range,\\n                                                              w_shift_range = w_shift_range,\\n                                                              rotation_range= rotation_range,\\n                                                              shear_range= shear_range,\\n                                                              zoom_range=zoom_range,\\n                                                              x_col=x_col,\\n                                                              y_col=y_col,\\n                                                              classes = categories,\\n                                                              factor=factor,\\n                                                              tgt_size=tgt_size,\\n                                                              batch_size=batch_size)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "gen_p_p_train,gen_p_p_train_size,gen_p_p_val,gen_p_p_val_size,gen_p_p_test,gen_p_p_test_size = generate_images(df=pneumonia_all_df,\n",
    "                                                       train_directory= train_directory,\n",
    "                                                       val_directory = val_directory,\n",
    "                                                       test_directory = test_directory,\n",
    "                                                              rescale=rescale,\n",
    "                                                              h_flip=h_flip,\n",
    "                                                              h_shift_range = h_shift_range,\n",
    "                                                              w_shift_range = w_shift_range,\n",
    "                                                              rotation_range= rotation_range,\n",
    "                                                              shear_range= shear_range,\n",
    "                                                              zoom_range=zoom_range,\n",
    "                                                              x_col=x_col,\n",
    "                                                              y_col=y_col,\n",
    "                                                              classes = categories,\n",
    "                                                              factor=factor,\n",
    "                                                              tgt_size=tgt_size,\n",
    "                                                              batch_size=batch_size)\n",
    "'''                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2.2 Augmentation pneumonia with other decease cases generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up GPU configuration\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor : 10\n",
      "train size = 286 = 1144*10//40\n",
      "batch size : 40\n",
      "/workspace/home/generated/PNEUMONIA/train\n",
      "Found 1144 validated image filenames belonging to 2 classes.\n",
      "batch size : 40\n",
      "/workspace/home/generated/PNEUMONIA/val\n",
      "Found 143 validated image filenames belonging to 2 classes.\n",
      "batch size : 40\n",
      "/workspace/home/generated/PNEUMONIA/test\n",
      "Found 143 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Due to memmory problem split reduce to 7\n",
    "#categories = ['NO_DECEASE','DECEASE_WO_PNEUMONIA',PNEUMONIA_AND_DECEASE'] \n",
    "train_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[1],'train')[0]\n",
    "val_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[1],'val')[0]\n",
    "test_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[1],'test')[0]\n",
    "\n",
    "gen_p_train,gen_p_train_size,gen_p_val,gen_p_val_size,gen_p_test,gen_p_test_size = generate_images(df=pneumonia_all_df,\n",
    "                                                       train_directory= train_directory,\n",
    "                                                       val_directory=val_directory,\n",
    "                                                       test_directory = test_directory,\n",
    "                                                                rescale = rescale,\n",
    "                                                                h_flip=h_flip,\n",
    "                                                                h_shift_range = h_shift_range,\n",
    "                                                                w_shift_range = w_shift_range,\n",
    "                                                                rotation_range= rotation_range,\n",
    "                                                                shear_range= shear_range,\n",
    "                                                                zoom_range=zoom_range,\n",
    "                                                                x_col=x_col,\n",
    "                                                                y_col=y_col,\n",
    "                                                                classes=categories,\n",
    "                                                                factor=factor,\n",
    "                                                                tgt_size=tgt_size,\n",
    "                                                                batch_size=batch_size)\n",
    "#set back the CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set back CPU\n",
    "#physical_devices = tf.config.list_physical_device('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3.Balance and generate of other decease and healthy cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.1 Function for data balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Replace 'your_dataset.csv' with your dataset file path and specify the column names and total_rows_to_select.\n",
    "# balanced_data = balance_dataset(df, ['age', 'gender'], 100)\n",
    "def select_balanced_dataset(df, column_names, total_rows_to_select):\n",
    "    # Calculate the total number of rows you want to select\n",
    "    num_groups = df.groupby(column_names).size().shape[0]\n",
    "    min_records_per_group = total_rows_to_select // num_groups\n",
    "    # Initialize an empty DataFrame to store the sampled rows\n",
    "    balanced_df = pd.DataFrame(columns=df.columns)\n",
    "    selected_indexes = []\n",
    "    groupes_count = {}\n",
    "    # Randomly sample from each group\n",
    "    for _, group in df.groupby(column_names):\n",
    "        count = group.shape[0]\n",
    "        if count >= min_records_per_group:\n",
    "            # If the group has enough records, sample the required number\n",
    "            sample = group.sample(min_records_per_group, random_state=42)\n",
    "        else:\n",
    "            # If the group has fewer records than required, sample all of them\n",
    "            sample = group    \n",
    "        # Add the sampled rows to the balanced DataFrame\n",
    "        balanced_df = pd.concat([balanced_df, sample])\n",
    "\n",
    "        #store the index\n",
    "        selected_indexes.extend(sample.index)\n",
    "        \n",
    "    # If you want a completely random subset, shuffle the resulting DataFrame\n",
    "    balanced_df = balanced_df.sample(frac=1, random_state=42)\n",
    "\n",
    "    return balanced_df , selected_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.2. Balanced selection of other than pneumonia desease cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickup  ~ 2 * generated pneumonia ( ~10000 training case and 3000 test case) ->  25000 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         NO_PNEUMONIA\n",
       "1         NO_PNEUMONIA\n",
       "2         NO_PNEUMONIA\n",
       "4         NO_PNEUMONIA\n",
       "5         NO_PNEUMONIA\n",
       "              ...     \n",
       "112096    NO_PNEUMONIA\n",
       "112097    NO_PNEUMONIA\n",
       "112100    NO_PNEUMONIA\n",
       "112106    NO_PNEUMONIA\n",
       "112108    NO_PNEUMONIA\n",
       "Name: category, Length: 50321, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decease_wo_pneumonia_df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_number = 20000\n",
    "#Due to memmory problem split reduce to 7\n",
    "#selection_number=20000\n",
    "#selection_number=10000\n",
    "\n",
    "balanced_decease_wo_pneumonia_df,selected_index = select_balanced_dataset(decease_wo_pneumonia_df, column_names = ['Patient Age','Patient Gender','View Position' ], total_rows_to_select = selection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15818"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_decease_wo_pneumonia_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>...</th>\n",
       "      <th>Edema</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Hernia</th>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <th>Infiltration</th>\n",
       "      <th>decease_number</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27344</th>\n",
       "      <td>00007120_004.png</td>\n",
       "      <td>4</td>\n",
       "      <td>7120</td>\n",
       "      <td>51</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO_PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81302</th>\n",
       "      <td>00019967_020.png</td>\n",
       "      <td>20</td>\n",
       "      <td>19967</td>\n",
       "      <td>6</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2544</td>\n",
       "      <td>3056</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.139</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NO_PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>00001946_031.png</td>\n",
       "      <td>31</td>\n",
       "      <td>1946</td>\n",
       "      <td>19</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2896</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.139</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NO_PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70747</th>\n",
       "      <td>00017446_002.png</td>\n",
       "      <td>2</td>\n",
       "      <td>17446</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2866</td>\n",
       "      <td>2621</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO_PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49940</th>\n",
       "      <td>00012643_002.png</td>\n",
       "      <td>2</td>\n",
       "      <td>12643</td>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.168</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NO_PNEUMONIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Image Index Follow-up # Patient ID Patient Age Patient Gender  \\\n",
       "27344  00007120_004.png           4       7120          51              F   \n",
       "81302  00019967_020.png          20      19967           6              M   \n",
       "7386   00001946_031.png          31       1946          19              M   \n",
       "70747  00017446_002.png           2      17446          41              F   \n",
       "49940  00012643_002.png           2      12643          70              F   \n",
       "\n",
       "      View Position OriginalImage[Width Height]  OriginalImagePixelSpacing[x  \\\n",
       "27344            PA                2048    2500                        0.168   \n",
       "81302            AP                2544    3056                        0.139   \n",
       "7386             AP                2896    2544                        0.139   \n",
       "70747            PA                2866    2621                        0.143   \n",
       "49940            AP                2500    2048                        0.168   \n",
       "\n",
       "          y]  ... Edema No Finding Effusion Pneumothorax Atelectasis Hernia  \\\n",
       "27344  0.168  ...     0          0        0            0           0      0   \n",
       "81302  0.139  ...     0          0        0            0           1      0   \n",
       "7386   0.139  ...     0          0        0            0           0      0   \n",
       "70747  0.143  ...     0          0        1            0           0      0   \n",
       "49940  0.168  ...     0          0        1            0           1      0   \n",
       "\n",
       "      Pleural_Thickening Infiltration decease_number      category  \n",
       "27344                  1            0              1  NO_PNEUMONIA  \n",
       "81302                  0            1              2  NO_PNEUMONIA  \n",
       "7386                   0            1              1  NO_PNEUMONIA  \n",
       "70747                  0            0              1  NO_PNEUMONIA  \n",
       "49940                  0            0              2  NO_PNEUMONIA  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_decease_wo_pneumonia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if balanced cross Gender,age and view postion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7779.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        8039.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_decease_wo_pneumonia_df['Patient Gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 624., 1548., 1980., 2200., 1980., 1980., 2200., 1965., 1157.,\n",
       "         184.]),\n",
       " array([1.0, 10.3, 19.6, 28.900000000000002, 38.2, 47.5,\n",
       "        56.800000000000004, 66.10000000000001, 75.4, 84.7, 94.0],\n",
       "       dtype=object),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_decease_wo_pneumonia_df['Patient Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** there to few sample for age < 20 and >=80 to have a perfect equal rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7822.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "        7996.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(balanced_decease_wo_pneumonia_df['View Position'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.3. Balanced selection of healthy cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to memmory problem split reduce to 7\n",
    "selection_number=20000\n",
    "#selection_number=10000\n",
    "\n",
    "balanced_nodecease_df,selected_index = select_balanced_dataset(nodecease_df, column_names = ['Patient Age','Patient Gender','View Position' ], total_rows_to_select = selection_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15964"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pickup  ~ 2 * generated pneumonia ( ~10000 taraining case and 3000 test case) ->  25000 cases\n",
    "\n",
    "len(balanced_nodecease_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31782\n"
     ]
    }
   ],
   "source": [
    "no_pneumonia_all_df= pd.concat([balanced_nodecease_df,balanced_decease_wo_pneumonia_df],ignore_index=True)\n",
    "#shuffle\n",
    "no_pneumonia_all_df = no_pneumonia_all_df.sample(frac=1).reset_index(drop=True)\n",
    "print(len(no_pneumonia_all_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3.3. Initialize parameter for images generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale= 1/255\n",
    "h_flip=False\n",
    "h_shift_range=0\n",
    "w_shift_range=0\n",
    "rotation_range=0\n",
    "shear_range=0\n",
    "zoom_range=0\n",
    "\n",
    "x_col = 'path'\n",
    "y_col = 'category'\n",
    "factor = 1\n",
    "## for VGG16\n",
    "#tgt_size = (224,224)\n",
    "#tgt_size = (299,299)\n",
    "#batch_size = 1\n",
    "#batch_size = 2*22\n",
    "batch_size = 2*2*22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categories = ['NO_DECEASE','DECEASE_WO_PNEUMONIA','PNEUMONIA_PURE_DECEASE','PNEUMONIA_OTHER_DECEASE'] \n",
    "train_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[0],'train')[0]\n",
    "val_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[0],'val')[0]\n",
    "test_directory = get_from_paths_dictonary(generated_file_paths_dic,categories[0],'test')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor : 1\n",
      "train size = 288 = 25426*1//88\n",
      "batch size : 88\n",
      "/workspace/home/generated/NO_PNEUMONIA/train\n",
      "Found 25426 validated image filenames belonging to 2 classes.\n",
      "batch size : 88\n",
      "/workspace/home/generated/NO_PNEUMONIA/val\n",
      "Found 3178 validated image filenames belonging to 2 classes.\n",
      "batch size : 88\n",
      "/workspace/home/generated/NO_PNEUMONIA/test\n",
      "Found 3178 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_o_train,gen_o_train_size,gen_o_val,gen_o_val_size,gen_o_test,gen_o_test_size = generate_images(df=no_pneumonia_all_df,\n",
    "                                                       train_directory= train_directory,\n",
    "                                                       val_directory = val_directory,\n",
    "                                                       test_directory = test_directory,\n",
    "                                                      rescale = rescale,\n",
    "                                                      h_flip=h_flip,\n",
    "                                                      h_shift_range = h_shift_range,\n",
    "                                                      w_shift_range = w_shift_range,\n",
    "                                                      rotation_range= rotation_range,\n",
    "                                                      shear_range= shear_range,\n",
    "                                                      zoom_range=zoom_range,\n",
    "                                                      x_col=x_col,\n",
    "                                                      y_col=y_col,\n",
    "                                                      classes=categories,\n",
    "                                                      factor=factor,\n",
    "                                                      tgt_size=tgt_size,\n",
    "                                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we won't complete with random data to 25000 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2.3.4. generate other than pneumonia iamges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set back the CPU configuration\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3. Preparation of mixed generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data setup:\n",
      "Generator for category: PNEUMONIA training size: 286, val size: 3, test size: 3\n",
      "Generator for category: NO_PNEUMONIA training size: 288, val size: 36, test size: 36\n",
      "train_limits [288, 286]\n",
      "Create Generator :train\n",
      "--0\n",
      "0\n",
      "mini train gen 0\n",
      "val limits [3, 36]\n",
      "Create Generator :validate\n",
      "mini val gen 1\n",
      "Ready to train\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Original data \n",
    "healthy cases                 : 60353\n",
    "Decease cases                 : 51751\n",
    "  Not pneumonia cases         :   50321\n",
    "  Pneumonia cases             :   1430\n",
    "    -Pure cases               :     322\n",
    "    -With other deceases cases:     1108\n",
    "Total cases  \n",
    "'''\n",
    "'''\n",
    "generator\n",
    "gen_p_o_train,gen_p_o_train_size,gen_p_o_test,gen_p_o_test_size\n",
    "gen_p_p_train,gen_p_p_train_size,gen_p_p_test,gen_p_p_test_size\n",
    "gen_o_train,gen_o_train_size,gen_o_test,gen_o_test_size\n",
    "gen_h_train,gen_h_train_size,gen_h_test,gen_h_test_size \n",
    "'''\n",
    "\n",
    "mini =True\n",
    "#mini =False\n",
    "\n",
    "print('Training data setup:')\n",
    "\n",
    "print(f'Generator for category: {categories[1]} training size: {gen_p_train_size}, val size: {gen_p_val_size}, test size: {gen_p_test_size}')\n",
    "print(f'Generator for category: {categories[0]} training size: {gen_o_train_size}, val size: {gen_o_val_size}, test size: {gen_o_test_size}')\n",
    "\n",
    "\n",
    "\n",
    "#train_gen = MixedImageDataGenerator([gen_p_o_train, gen_p_p_train,gen_o_train,gen_h_train],\n",
    "#                                        [gen_p_o_train_size, gen_p_p_train_size,gen_o_train_size,gen_h_train_size])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_generators =[gen_o_train,gen_p_train]\n",
    "train_limits =  [gen_o_train_size,gen_p_train_size]\n",
    "print(f'train_limits {train_limits}')\n",
    "\n",
    "#mini_train_limits = [80,80,80]\n",
    "mini_train_limits = [10,10]\n",
    "train_gen = MixedImageDataGenerator(train_generators,train_limits,MixedImageDataGenerator.TRAIN)\n",
    "\n",
    "mini_train_gen = MixedImageDataGenerator(train_generators,mini_train_limits,MixedImageDataGenerator.TRAIN,debug=True)\n",
    "print(f'--{MixedImageDataGenerator.TRAIN}')\n",
    "print(MixedImageDataGenerator.TRAIN)\n",
    "print(f'mini train gen {mini_train_gen.getType()}')\n",
    "\n",
    "\n",
    "#val_gen = MixedImageDataGenerator([gen_p_o_val, gen_p_p_val,gen_o_val,gen_h_val],\n",
    "#[gen_p_o_val_size, gen_p_p_val_size,gen_o_val_size,gen_h_val_size])\n",
    "\n",
    "val_generators =[gen_p_val,gen_o_val]\n",
    "val_limits = [gen_p_val_size, gen_o_val_size]\n",
    "print(f'val limits {val_limits}')\n",
    "\n",
    "\n",
    "val_gen = MixedImageDataGenerator(val_generators,val_limits,MixedImageDataGenerator.VALIDATE)\n",
    "\n",
    "mini_val_limits = [10,10]\n",
    "mini_val_gen = MixedImageDataGenerator(val_generators,mini_val_limits,MixedImageDataGenerator.VALIDATE,debug=True)\n",
    "print(f'mini val gen {mini_val_gen.getType()}')\n",
    "\n",
    "\n",
    "data_refresher_callback = MixedImageDataGeneratorRefresherCallBack([train_gen,val_gen])\n",
    "mini_data_refresher_callback = MixedImageDataGeneratorRefresherCallBack([mini_train_gen,mini_val_gen])\n",
    "\n",
    "\n",
    "#test_gen = MixedImageDataGenerator([gen_p_o_test, gen_p_p_test,gen_o_test,gen_h_test],\n",
    "#  [gen_p_o_test_size, gen_p_p_test_size,gen_o_test_size,gen_h_test_size])\n",
    "test_generators =[gen_p_test,gen_o_test]\n",
    "test_limits =   [gen_p_test_size, gen_o_test_size]\n",
    "test_gen = MixedImageDataGenerator(test_generators, test_limits,MixedImageDataGenerator.TEST,)\n",
    "mini_test_gen = MixedImageDataGenerator(test_generators, [10,10,10],MixedImageDataGenerator.TEST)\n",
    "\n",
    "      \n",
    "        \n",
    "print('Ready to train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test \n",
    "\n",
    "\n",
    "#left_negative_indexes = [int(i) for i in all_negative_indexes if i not in selected_negative_balanced_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_left_negative_indexes = np.random.choice(left_negative_indexes,220 - len(negative_balanced_df) ,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nchecked_selected_left_negative_indexes=[]\\nfor i in range(len(selected_left_negative_indexes)):\\n    if selected_left_negative_indexes[i] in data_negative_pneumothorax.index:\\n        checked_selected_left_negative_indexes.append(selected_left_negative_indexes[i])\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check because of weird positional indexers are out-of-bounds exception\n",
    "'''\n",
    "checked_selected_left_negative_indexes=[]\n",
    "for i in range(len(selected_left_negative_indexes)):\n",
    "    if selected_left_negative_indexes[i] in data_negative_pneumothorax.index:\n",
    "        checked_selected_left_negative_indexes.append(selected_left_negative_indexes[i])\n",
    "'''        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left_negative_balanced_df = data_negative_pneumothorax.loc[checked_selected_left_negative_indexes]\n",
    "#left_negative_balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_and_test_data  = negative_balanced_df.merge(data_positive_pneumothorax,how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can begin our model-building & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First suggestion: perform some image augmentation on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Index</th>\n",
       "      <th>Follow-up #</th>\n",
       "      <th>Patient ID</th>\n",
       "      <th>Patient Age</th>\n",
       "      <th>Patient Gender</th>\n",
       "      <th>View Position</th>\n",
       "      <th>OriginalImage[Width</th>\n",
       "      <th>Height]</th>\n",
       "      <th>OriginalImagePixelSpacing[x</th>\n",
       "      <th>y]</th>\n",
       "      <th>...</th>\n",
       "      <th>Edema</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Hernia</th>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <th>Infiltration</th>\n",
       "      <th>decease_number</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>00000061_015.png</td>\n",
       "      <td>15</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>00000144_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "      <td>83</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>00000165_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>76</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2992</td>\n",
       "      <td>2991</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>00000193_019.png</td>\n",
       "      <td>19</td>\n",
       "      <td>193</td>\n",
       "      <td>55</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>2500</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>00000218_001.png</td>\n",
       "      <td>1</td>\n",
       "      <td>218</td>\n",
       "      <td>33</td>\n",
       "      <td>M</td>\n",
       "      <td>PA</td>\n",
       "      <td>2048</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107205</th>\n",
       "      <td>00028924_005.png</td>\n",
       "      <td>5</td>\n",
       "      <td>28924</td>\n",
       "      <td>72</td>\n",
       "      <td>F</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108694</th>\n",
       "      <td>00029481_004.png</td>\n",
       "      <td>4</td>\n",
       "      <td>29481</td>\n",
       "      <td>51</td>\n",
       "      <td>F</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109877</th>\n",
       "      <td>00029889_000.png</td>\n",
       "      <td>0</td>\n",
       "      <td>29889</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>PA</td>\n",
       "      <td>2021</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.194311</td>\n",
       "      <td>0.194311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110426</th>\n",
       "      <td>00030079_018.png</td>\n",
       "      <td>18</td>\n",
       "      <td>30079</td>\n",
       "      <td>16</td>\n",
       "      <td>M</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111767</th>\n",
       "      <td>00030621_002.png</td>\n",
       "      <td>2</td>\n",
       "      <td>30621</td>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>AP</td>\n",
       "      <td>3056</td>\n",
       "      <td>2544</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PNEUMONIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Image Index  Follow-up #  Patient ID  Patient Age Patient Gender  \\\n",
       "279     00000061_015.png           15          61           77              M   \n",
       "590     00000144_001.png            1         144           83              M   \n",
       "640     00000165_001.png            1         165           76              M   \n",
       "804     00000193_019.png           19         193           55              M   \n",
       "902     00000218_001.png            1         218           33              M   \n",
       "...                  ...          ...         ...          ...            ...   \n",
       "107205  00028924_005.png            5       28924           72              F   \n",
       "108694  00029481_004.png            4       29481           51              F   \n",
       "109877  00029889_000.png            0       29889           44              F   \n",
       "110426  00030079_018.png           18       30079           16              M   \n",
       "111767  00030621_002.png            2       30621           22              F   \n",
       "\n",
       "       View Position  OriginalImage[Width  Height]  \\\n",
       "279               AP                 3056     2544   \n",
       "590               AP                 2500     2048   \n",
       "640               PA                 2992     2991   \n",
       "804               AP                 2500     2048   \n",
       "902               PA                 2048     2500   \n",
       "...              ...                  ...      ...   \n",
       "107205            AP                 3056     2544   \n",
       "108694            AP                 3056     2544   \n",
       "109877            PA                 2021     2021   \n",
       "110426            AP                 3056     2544   \n",
       "111767            AP                 3056     2544   \n",
       "\n",
       "        OriginalImagePixelSpacing[x        y]  ... Edema  No Finding  \\\n",
       "279                        0.139000  0.139000  ...     0           0   \n",
       "590                        0.168000  0.168000  ...     0           0   \n",
       "640                        0.143000  0.143000  ...     0           0   \n",
       "804                        0.168000  0.168000  ...     0           0   \n",
       "902                        0.171000  0.171000  ...     0           0   \n",
       "...                             ...       ...  ...   ...         ...   \n",
       "107205                     0.139000  0.139000  ...     0           0   \n",
       "108694                     0.139000  0.139000  ...     0           0   \n",
       "109877                     0.194311  0.194311  ...     0           0   \n",
       "110426                     0.139000  0.139000  ...     0           0   \n",
       "111767                     0.139000  0.139000  ...     0           0   \n",
       "\n",
       "        Effusion  Pneumothorax  Atelectasis  Hernia  Pleural_Thickening  \\\n",
       "279            0             0            0       0                   0   \n",
       "590            0             0            0       0                   0   \n",
       "640            0             0            0       0                   0   \n",
       "804            0             0            0       0                   0   \n",
       "902            0             0            0       0                   0   \n",
       "...          ...           ...          ...     ...                 ...   \n",
       "107205         0             0            0       0                   0   \n",
       "108694         0             0            0       0                   0   \n",
       "109877         0             0            0       0                   0   \n",
       "110426         0             0            0       0                   0   \n",
       "111767         0             0            0       0                   0   \n",
       "\n",
       "        Infiltration  decease_number   category  \n",
       "279                0               1  PNEUMONIA  \n",
       "590                0               1  PNEUMONIA  \n",
       "640                0               1  PNEUMONIA  \n",
       "804                0               1  PNEUMONIA  \n",
       "902                0               1  PNEUMONIA  \n",
       "...              ...             ...        ...  \n",
       "107205             0               1  PNEUMONIA  \n",
       "108694             0               1  PNEUMONIA  \n",
       "109877             0               1  PNEUMONIA  \n",
       "110426             0               1  PNEUMONIA  \n",
       "111767             0               1  PNEUMONIA  \n",
       "\n",
       "[322 rows x 28 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pneumonia_pure_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to pull a single large batch of random validation data for testing after each epoch:\n",
    "##valX, valY = val_gen.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your model: \n",
    "\n",
    "Recommendation here to use a pre-trained network downloaded from Keras for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator for category: PNEUMONIA weight: 0.49825783972125437\n",
      "Generator for category: NO_PNEUMONIA weight: 0.5017421602787456\n",
      "Total generation: 574\n",
      "~  2/3 no pnemonary cases , 1/3 pneumonia\n"
     ]
    }
   ],
   "source": [
    "#compte weight of each category\n",
    "gen_tot_size = gen_p_train_size   + gen_o_train_size\n",
    "# pneumonia_pure_data_size\n",
    "# pneumonia_other_data_size\n",
    "w_p = gen_p_train_size/gen_tot_size\n",
    "w_o = gen_o_train_size/gen_tot_size\n",
    "\n",
    "#categories = ['NO_DECEASE','DECEASE_WO_PNEUMONIA','PNEUMONIA_PURE_DECEASE','PNEUMONIA_OTHER_DECEASE'] \n",
    "\n",
    "print(f'Generator for category: {categories[1]} weight: {w_p}')\n",
    "print(f'Generator for category: {categories[0]} weight: {w_o}')\n",
    "print(f'Total generation: {gen_tot_size}')\n",
    "\n",
    "print('~  2/3 no pnemonary cases , 1/3 pneumonia' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense,Dropout,GlobalAveragePooling2D,AveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def create_custom_VGG16_V2(num_classes,dropout=0.5,input_shape= (224,224,3)):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    base_model = tf.keras.applications.vgg16.VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model_vgg16 = tf.keras.Sequential()\n",
    "    model_vgg16.add(base_model)\n",
    "    model_vgg16.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "    model_vgg16.add(tf.keras.layers.Flatten())\n",
    "    model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model_vgg16.add(tf.keras.layers.Dropout(0.5))\n",
    "    model_vgg16.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model_vgg16.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    model_vgg16.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    return model_vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense,Dropout,GlobalAveragePooling2D,AveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def create_custom_VGG16(num_classes,dropout=0.5,input_shape= (224,224,3)):\n",
    "    \n",
    "    # Load the pre-trained VGG16 model without the top (fully connected) layers\n",
    "    base_model = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Add custom top layers\n",
    "    \n",
    "    ''' We choose (helped by chatGPT)\n",
    "        -GlobalAveragePooling2D reduces the spatial dimensions of the feature maps to a 1x1 size, \n",
    "         effectively summarizing the entire feature map into a single value per channel. \n",
    "         This can help in capturing the most important features while discarding less relevant spatial information.\n",
    "        -With GlobalAveragePooling2D, the model becomes more robust to translations and small spatial distortions\n",
    "         in the input images, which could be helpful in real-world scenarios where the positioning of the disease \n",
    "         within the image might vary.\n",
    "        -It significantly reduces the number of parameters compared to Flatten, which can aid in faster \n",
    "         training and inference times\n",
    "    \n",
    "    '''\n",
    "    '''\n",
    "    x=None\n",
    "    if global_average_pooling2D:\n",
    "        x = GlobalAveragePooling2D()(base_model.output)    \n",
    "    else:\n",
    "        x = Flatten()(base_model.output)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    predictions = Dense(categories_nbr, activation='softmax')(x)\n",
    "    vgg_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    '''\n",
    "    # Freeze the pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    # Add custom top layers\n",
    "    x = AveragePooling2D(pool_size=(4, 4))(base_model.output)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    #predictions = Dense(1, activation='sigmoid')(x)  # Use a single neuron with sigmoid activation for binary classification\n",
    "    predictions = Dense(2, activation='softmax')(x)  # Use a single neuron with sigmoid activation for binary classification\n",
    "    # Create the model\n",
    "    vgg_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    # optimizer = Adam(lr=0.001)  # You can adjust the learning rate\n",
    "    # vgg_model.compile(optimizer=optimizer,\n",
    "    #          loss=SparseCategoricalCrossentropy(),\n",
    "    #          metrics=['accuracy'])\n",
    "    \n",
    "    return vgg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "def create_custom_resnet50v2(num_classes,dropout=0.3,input_shape= (224,224,3)):\n",
    "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze all layers of the ResNet50V2 base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom classifier layers\n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    predictions = Dense(2, activation='sofmax')(x)  # Use a single neuron with sigmoid activation for binary classification\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "def create_custom_inceptionv3(num_classes,dropout=0.3,input_shape= (224,224,3)):\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze all layers of the InceptionV3 base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add custom classifier layers\n",
    "    x = Flatten()(x)  # Flatten the output of the convolutional layers\n",
    "    x = Dense(1024, activation=\"relu\")(x)  # Add a fully connected layer with ReLU activation\n",
    "    predictions = Dense(num_classes, activation=\"softmax\")(x)  # Output layer with softmax for probabilities\n",
    "    model = Model(inputs=base_model.input, outputs=prediction)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 15,895,618\n",
      "Trainable params: 1,180,930\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "channel_nbr = 3\n",
    "model_name = 'VGG'\n",
    "my_model = create_custom_VGG16(len(categories),dropout=0.3,input_shape = (tgt_size[0],tgt_size[1],channel_nbr))\n",
    "#model_name = 'INC3'\n",
    "#my_model = create_custom_inceptionv3(len(categories),dropout=0.3,input_shape = (tgt_size[0],tgt_size[1],channel_nbr))\n",
    "#model_name = 'RES50V2'\n",
    "#my_model = create_custom_resnet50v2(len(categories),dropout=0.5,input_shape = (tgt_size[0],tgt_size[1],channel_nbr))\n",
    "\n",
    "# display the structure of the model\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VGG'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLossCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.x_train = self.model.train_data[0]\n",
    "        self.y_train = self.model.train_data[1]\n",
    "        if hasattr(self.model, 'val_data'):\n",
    "            self.x_val = self.model.val_data[0]\n",
    "            self.y_val = self.model.val_data[1]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Compute and store metrics for training data\n",
    "        train_loss, train_accuracy = self.compute_metrics(self.x_train, self.y_train)\n",
    "        self.model.history.history['train_custom_loss'] = train_loss\n",
    "        self.model.history.history['train_custom_accuracy'] = train_accuracy\n",
    "\n",
    "        # Compute and store metrics for validation data if provided\n",
    "        if hasattr(self, 'x_val') and hasattr(self, 'y_val'):\n",
    "            val_loss, val_accuracy = self.compute_metrics(self.x_val, self.y_val)\n",
    "            self.model.history.history['val_custom_loss'] = val_loss\n",
    "            self.model.history.history['val_custom_accuracy'] = val_accuracy\n",
    "\n",
    "    def compute_metrics(self, x, y):\n",
    "        y_pred = self.model.predict(x)\n",
    "        y_pred_classes = tf.cast(tf.argmax(y_pred, axis=1), dtype=tf.float32)\n",
    "        print(y_pred_classes)\n",
    "        print(type(y_pred_classes))\n",
    "        print(y)\n",
    "        print(type(y))\n",
    "     \n",
    "        # Compute loss\n",
    "        loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        loss = loss_function(y, y_pred_classes)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(y, y_pred_classes), tf.float32))\n",
    "        \n",
    "        return loss.numpy(), accuracy.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch,initial_lr =0.01,min_lr = 0.0001,decay_rate = 0.9):\n",
    "    lr = initial_lr * decay_rate**(epoch)\n",
    "    if lr< min_lr:\n",
    "        return min_lr\n",
    "    else:\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Iterate on all underlyne generators once, and retrun compined result.\n",
    "\n",
    "Parameters:\n",
    "    generator (MixedImageDataGenerator): \n",
    "    shuffle (boolean): If true shuffle\n",
    "\n",
    "    Returns:\n",
    "    tuple of numpy.array: combined_features_array, combined_labels_array   \n",
    "'''\n",
    "def get_step_dataset_from_generator(generator, shuffle=False):\n",
    "    \n",
    "    #prepare validation data\n",
    "    feature_batches = []\n",
    "    label_batches = []\n",
    "    for i in range(generator.get_generator_number()):\n",
    "        try:\n",
    "            batch = next(generator)\n",
    "            feature_batches.append(batch[0])  # Assuming features are at index 0\n",
    "            label_batches.append(batch[1])    # Assuming labels are at index 1\n",
    "        except  StopIteration:  \n",
    "            print(f'[WARNING] Generator {i} exhausted')\n",
    " \n",
    "    combined_features = None\n",
    "    combined_labels =None\n",
    "    l =   len(feature_batches)\n",
    "    if l>0:\n",
    "        print(f'feature_batches; {l}')\n",
    "        combined_features= np.concatenate(feature_batches, axis=0)\n",
    "    l = len(label_batches)\n",
    "    if  l >0:\n",
    "        print(f'label_batches; {l}')\n",
    "        combined_labels = np.concatenate(label_batches, axis=0)\n",
    "    combined_labels_array = None\n",
    "    combined_features_array =None\n",
    "\n",
    "    if shuffle and combined_features is not None and len(combined_features) > 0:\n",
    "        #Shuffle  data\n",
    "        combined_data = list(zip(combined_features, combined_labels))\n",
    "        np.random.shuffle(combined_data)\n",
    "        combined_features, combined_labels = zip(*combined_data)\n",
    "    # convert to numpay array    \n",
    "    if combined_features is not None and len(combined_features)>0:\n",
    "        combined_features_array= np.array([(np.array(arr)) for arr in combined_features])\n",
    "    if combined_labels is not None and len(combined_labels)>0:\n",
    "        combined_labels_array = np.array([(np.array(arr)) for arr in combined_labels] )\n",
    "            \n",
    "    return combined_features_array, combined_labels_array   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, initial_lr=0.1, decay_rate=0.9):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.lr = initial_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.lr = self.initial_lr * self.decay_rate**(epoch)\n",
    "        print(f'lr {self.lr}')\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.lr)\n",
    "        \n",
    "    def get_learning_rate(self):\n",
    "        return self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Custom callback to log epoch and step information, and refresh the generator.\n",
    "\"\"\"\n",
    "class EpochStepLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, generator):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          generator: The data generator object to be refreshed.\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.current_epoch = 0\n",
    "        self.current_step = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        Called at the beginning of each epoch.\n",
    "\n",
    "        Args:\n",
    "          epoch: The current epoch number.\n",
    "          logs: Dictionary of logs accumulated during the previous epoch.\n",
    "        \"\"\"\n",
    "        self.current_epoch = epoch\n",
    "        self.current_step = 0\n",
    "        self.log(f\"Epoch {epoch} started!\")\n",
    "        self.refresh_generator()\n",
    "\n",
    "        \n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        \"\"\"\n",
    "        Called at the end of each batch.\n",
    "\n",
    "        Args:\n",
    "          batch: The index of the current batch.\n",
    "          logs: Dictionary of logs accumulated at the end of the current batch.\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        self.log(f\"Epoch {self.current_epoch}, Step {self.current_step} completed.\")\n",
    "        # in case where there is more than one call to next() per step\n",
    "        # if self.generator:\n",
    "        #   if self.generator.get_remaining_batches() < 1:\n",
    "        #     self.refresh_generator()\n",
    "        \n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"\n",
    "        Called at the end of training.\n",
    "\n",
    "        Args:\n",
    "          logs: Dictionary of logs accumulated at the end of training.\n",
    "        \"\"\"\n",
    "        self.log(\"Training completed!\")\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"\n",
    "        Called at the begining of training.\n",
    "\n",
    "        Args:\n",
    "          logs: Dictionary of logs accumulated at the end of training.\n",
    "        \"\"\"\n",
    "        self.refresh_generator()\n",
    "        self.log(\"Training started!\")    \n",
    "\n",
    "    def refresh_generator(self):\n",
    "        \"\"\"\n",
    "        Refreshes the data generator.\n",
    "        \"\"\"\n",
    "        if self.generator:\n",
    "            self.generator.refresh()  # Assuming your generator has a refresh method\n",
    "\n",
    "    def log(self, message):\n",
    "        \"\"\"\n",
    "        Logs a message with epoch and step information.\n",
    "        \"\"\"\n",
    "        print(f\"[Epoch {self.current_epoch}, Step {self.current_step}] {message}\")\n",
    "        \n",
    "    def on_test_begin(self,logs=None):\n",
    "        print(f'[ON_TEST_BEGIN]')\n",
    "    \n",
    "    def on_test_end(self,logs=None):\n",
    "        print(f'[ON_TEST_END]')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to look at some examples of our augmented training data. \n",
    "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
    "## and can be compared with how the raw data look prior to augmentation\n",
    "show_image = False\n",
    "if show_image:\n",
    "    mini_val_gen.refresh()\n",
    "    training_features, training_labels = get_step_dataset_from_generator(mini_val_gen, shuffle=True)\n",
    "    fig, m_axs = plt.subplots(4, 2, figsize = (20, 20))\n",
    "    for (c_x, c_y, c_ax) in zip(training_features, training_labels, m_axs.flatten()):\n",
    "        c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "        if c_y == 1: \n",
    "            c_ax.set_title('Pneumonia')\n",
    "        else:\n",
    "            c_ax.set_title('No Pneumonia')\n",
    "        c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] refresh\n",
      "[validate] refresh\n",
      "START TRAINING FOR 15 epochs for each batch [mode mini : True] ...\n",
      "START TRAINING prepare data for batch  0 on 15 epochs...\n",
      "Learning rate: 0.01\n",
      "[train]: next called [pre lock]\n",
      "[train]: next call ...\n",
      "[train]Remaining batch:  [9, 10] limits: [10, 10]\n",
      "[train]: RETURN <class 'tuple'>\n",
      "[train]: next called\n",
      "[train]: next called [pre lock]\n",
      "[train]: next call ...\n",
      "[train]Remaining batch:  [9, 9] limits: [10, 10]\n",
      "[train]: RETURN <class 'tuple'>\n",
      "[train]: next called\n",
      "feature_batches; 2\n",
      "label_batches; 2\n",
      "[validate]: next called [pre lock]\n",
      "[validate]: next call ...\n",
      "[validate]Remaining batch:  [9, 10] limits: [10, 10]\n",
      "[validate]: RETURN <class 'tuple'>\n",
      "[validate]: next called\n",
      "[validate]: next called [pre lock]\n",
      "[validate]: next call ...\n",
      "[validate]Remaining batch:  [9, 9] limits: [10, 10]\n",
      "[validate]: RETURN <class 'tuple'>\n",
      "[validate]: next called\n",
      "feature_batches; 2\n",
      "label_batches; 2\n",
      "START TRAINING for batch  0 on 15 epochs...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 128 samples, validate on 128 samples\n",
      "[Epoch 0, Step 0] Training started!\n",
      "[Epoch 0, Step 0] Epoch 0 started!\n",
      "Epoch 1/15\n",
      "[Epoch 0, Step 1] Epoch 0, Step 1 completed.\n",
      "[Epoch 0, Step 2] Epoch 0, Step 2 completed.\n",
      "[Epoch 0, Step 3] Epoch 0, Step 3 completed.\n",
      "[Epoch 0, Step 4] Epoch 0, Step 4 completed.\n",
      "[Epoch 0, Step 5] Epoch 0, Step 5 completed.\n",
      "[Epoch 0, Step 6] Epoch 0, Step 6 completed.\n",
      "[Epoch 0, Step 7] Epoch 0, Step 7 completed.\n",
      "[Epoch 0, Step 8] Epoch 0, Step 8 completed.\n",
      "[ON_TEST_BEGIN]\n",
      "[ON_TEST_END]\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35113, saving model to B_0_VGG_chest_ray_F_2XDO_512_best.h5\n",
      "128/128 - 348s - loss: 1.9548 - accuracy: 0.6094 - val_loss: 0.3511 - val_accuracy: 0.6875\n",
      "[Epoch 1, Step 0] Epoch 1 started!\n",
      "Epoch 2/15\n",
      "[Epoch 1, Step 1] Epoch 1, Step 1 completed.\n",
      "[Epoch 1, Step 2] Epoch 1, Step 2 completed.\n",
      "[Epoch 1, Step 3] Epoch 1, Step 3 completed.\n",
      "[Epoch 1, Step 4] Epoch 1, Step 4 completed.\n",
      "[Epoch 1, Step 5] Epoch 1, Step 5 completed.\n",
      "[Epoch 1, Step 6] Epoch 1, Step 6 completed.\n",
      "[Epoch 1, Step 7] Epoch 1, Step 7 completed.\n",
      "[Epoch 1, Step 8] Epoch 1, Step 8 completed.\n",
      "[ON_TEST_BEGIN]\n",
      "[ON_TEST_END]\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35113 to 0.28251, saving model to B_0_VGG_chest_ray_F_2XDO_512_best.h5\n",
      "128/128 - 347s - loss: 0.4004 - accuracy: 0.6406 - val_loss: 0.2825 - val_accuracy: 0.6875\n",
      "[Epoch 2, Step 0] Epoch 2 started!\n",
      "Epoch 3/15\n",
      "[Epoch 2, Step 1] Epoch 2, Step 1 completed.\n",
      "[Epoch 2, Step 2] Epoch 2, Step 2 completed.\n",
      "[Epoch 2, Step 3] Epoch 2, Step 3 completed.\n",
      "[Epoch 2, Step 4] Epoch 2, Step 4 completed.\n",
      "[Epoch 2, Step 5] Epoch 2, Step 5 completed.\n",
      "[Epoch 2, Step 6] Epoch 2, Step 6 completed.\n",
      "[Epoch 2, Step 7] Epoch 2, Step 7 completed.\n",
      "[Epoch 2, Step 8] Epoch 2, Step 8 completed.\n",
      "[ON_TEST_BEGIN]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 15\n",
    "if mini:\n",
    "    epochs = 15 \n",
    " \n",
    "stop_limit = 5\n",
    "early = EarlyStopping(monitor='val_loss', \n",
    "                      patience=5, \n",
    "                      restore_best_weights=True)\n",
    "\n",
    "\n",
    "# Assuming class proportions\n",
    "proportions = [2/3,1/3]\n",
    "               \n",
    "class_weight=dict(enumerate(proportions))\n",
    "\n",
    "#print(f'Generator for category: {categories[3]} weight: {w_p_o}')\n",
    "#print(f'Generator for category: {categories[0]} weight: {w_h}')\n",
    "#print(f'Generator for category: {categories[2]} weight: {w_p_p}')\n",
    "#print(f'Generator for category: {categories[1]} weight: {w_o}')\n",
    "#customLossCallback = CustomLossCallback()\n",
    "\n",
    "training_loss = []\n",
    "training_customized_loss = []\n",
    "training_accuracy =[]\n",
    "training_customized_accuracy =[]\n",
    "\n",
    "validation_loss = []\n",
    "validation_customized_loss = []\n",
    "validation_accuracy =[]\n",
    "validation_customized_accuracy =[]\n",
    "\n",
    "training_data = train_gen\n",
    "validation_data = val_gen\n",
    "\n",
    "if mini:\n",
    "    # get your model \n",
    "    training_data = mini_train_gen\n",
    "    validation_data = mini_val_gen\n",
    "\n",
    "# Init generator    \n",
    "training_data.refresh()    \n",
    "validation_data.refresh()\n",
    "\n",
    "lr = LearningRateDecay(initial_lr=0.1, decay_rate=0.9)\n",
    "epoch_step_logger = EpochStepLogger(None)\n",
    "\n",
    "    \n",
    "    \n",
    "print(f'START TRAINING FOR {epochs} epochs for each batch [mode mini : {mini}] ...')    \n",
    "    \n",
    "# Compile your model with custom learning rate\n",
    "#optimizer = Adam(learning_rate=0.01)\n",
    "#my_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "#                  # loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "#                  optimizer=optimizer,\n",
    "#                  metrics=['sparse_categorical_accuracy'])\n",
    "i = 0\n",
    "learning_rate = 0.01\n",
    "min_learning_rate = learning_rate/100\n",
    "num_stop = 0\n",
    "exit = False\n",
    "lr_prim =learning_rate\n",
    "while(not exit or training_features is not None):\n",
    "    \n",
    "    print(f'START TRAINING prepare data for batch  {i} on {epochs} epochs...')         \n",
    "    #lr = LearningRateDecay(initial_lr=learning_rate, decay_rate=0.9)\n",
    "    print(f'Learning rate: {lr_prim}')\n",
    "    optimizer = Adam(learning_rate=lr_prim)\n",
    "    my_model.compile(loss=SparseCategoricalCrossentropy(),  # Change loss function here\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    weight_path=\"{}_chest_ray_F_2XDO_512_best.h5\".format(f'B_{i}_{model_name}')\n",
    "    checkpoint = ModelCheckpoint(weight_path, \n",
    "                              monitor= 'val_loss', \n",
    "                              verbose=2, \n",
    "                              save_best_only=True, \n",
    "                              mode= 'min', \n",
    "                              save_weights_only = False)\n",
    "    # callbacks_list = [checkpoint, early,customLossCallback]  \n",
    "    # callbacks_list = [checkpoint, early,epoch_step_logger]  \n",
    "    #callbacks_list = [checkpoint, early,lr]  \n",
    "    callbacks_list = [checkpoint, early,epoch_step_logger]  \n",
    "    \n",
    "    # Generate batches for training data\n",
    "    training_features, training_labels = get_step_dataset_from_generator(training_data, shuffle=True)\n",
    "    validation_features, validation_labels = get_step_dataset_from_generator(validation_data, shuffle=False)\n",
    "    if validation_features is None:\n",
    "        # replay the valuidation data\n",
    "        print(f'Refresh validation data')\n",
    "        validation_data.refresh()\n",
    "        validation_features, validation_labels = get_step_dataset_from_generator(validation_data, shuffle=False)\n",
    "\n",
    "         \n",
    "    if training_features is None:\n",
    "        print(f'STOP TRAINING at batch  {i}, no more training data')         \n",
    "        exit = True\n",
    "        break\n",
    "    else:\n",
    "        \n",
    "        print(f'START TRAINING for batch  {i} on {epochs} epochs...')          \n",
    "        history = my_model.fit(x=[training_features],\n",
    "                               y=[training_labels.astype(np.int32)],\n",
    "                                validation_data=(validation_features,validation_labels.astype(np.int32)),\n",
    "                                epochs=epochs,\n",
    "                                steps_per_epoch =8,\n",
    "                                callbacks = callbacks_list,\n",
    "                                class_weight=class_weight,\n",
    "                                #batch_size = training_features.shape[0],\n",
    "                                verbose=2)\n",
    "        #learning_rate = lr.get_learning_rate()\n",
    "        #append metrics      \n",
    "        training_loss.append(history.history['loss'])\n",
    "        training_accuracy.append(history.history['accuracy'])\n",
    "        if 'train_custom_loss' in history.history:\n",
    "            train_custom_loss = history.history['train_custom_loss']\n",
    "            training_customized_loss.append(train_custom_loss)\n",
    "            train_custom_accuracy= history.history['train_custom_accuracy']\n",
    "            training_customized_accuracy.append(train_custom_accuracy)\n",
    "            print(f'train_custom_loss: {train_custom_loss} - train_custom_accuracy: {train_custom_accuracy}')\n",
    "        if 'val_loss' in history.history:\n",
    "            validation_loss.append(history.history['val_loss'])\n",
    "            validation_accuracy.append(history.history['val_accuracy'])\n",
    "        if 'val_custom_loss' in history.history:\n",
    "            validation_loss.append(history.history['val_custom_loss'])\n",
    "            validation_accuracy.append(history.history['val_custom_accuracy'])    \n",
    "            print(f'END TRAINING batch  {i} done on {epochs} epochs')         \n",
    "        if early.stopped_epoch > 0:\n",
    "            print(f\"STOP Early stopping triggered at segment {i}\")\n",
    "            num_stop+=1\n",
    "        #if num_stop>stop_limit:    \n",
    "        #    print(f\"STOP Early stopping at segment {i} after {num_stop} > {stop_limit}\")\n",
    "        #    exit= True\n",
    "        #    break\n",
    "        \n",
    "        lr_prim = lr_decay(i+1,initial_lr = learning_rate,min_lr =min_learning_rate, decay_rate = 0.98)\n",
    "        i += 1\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time        \n",
    "print(f'END TRAINING FOR {epochs} epochs in {total_time} seconds')      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Access additional metrics if present\n",
    "# additional_metric = history.history['your_additional_metric']\n",
    "\n",
    "# Example: Plotting training and validation loss over epochs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_range = range(1, len(training_loss)* epochs + 1)\n",
    "training_loss_flat = [item for sublist in training_loss for item in sublist]\n",
    "validation_loss_flat = [item for sublist in validation_loss for item in sublist]\n",
    "\n",
    "plt.figure (figsize = (12,10))\n",
    "plt.plot(range(len(training_loss_flat)),training_loss_flat, label='Training Loss')\n",
    "plt.plot(range(len(training_loss_flat)),  validation_loss_flat, label='Validation Loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure (figsize = (12,10))\n",
    "\n",
    "training_accuracy_flat = [item for sublist in training_accuracy for item in sublist]\n",
    "validation_accuracy_flat = [item for sublist in validation_accuracy for item in sublist]\n",
    "plt.plot(range(len(training_loss_flat)),  training_accuracy_flat, label='Training Accuracy')\n",
    "plt.plot(range(len(training_loss_flat)), validation_accuracy_flat, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx= validation_loss.index(min(validation_loss))\n",
    "print(\"Index of the minimum value:\", min_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    \n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC? \n",
    "\n",
    "\n",
    "# def ... \n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "    \n",
    "#Also consider plotting the history of your model training:\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of true vs. predicted with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "selected_model=load_model('/workspace/home/B_2_RES50V2_chest_ray_F_2XDO_512_best.h5')\n",
    "model_json = selected_model.to_json()\n",
    "with open(\"RES50V2_F_2XDO_512_best.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_model._is_compiled:\n",
    "     print('selected_model model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve,auc,recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "# Initialize lists to store true labels and predicted labels\n",
    "y_trues = []\n",
    "y_preds = []\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "# Initialize a flag to indicate if there is more data\n",
    "is_data = True\n",
    "mini =False\n",
    "#mini = True\n",
    "\n",
    "test_gen.refresh()\n",
    "\n",
    "test_generator= None\n",
    "if mini:\n",
    "    test_generator = mini_test_gen\n",
    "else:    \n",
    "    test_generator = test_gen\n",
    "\n",
    "test_generator.refresh()\n",
    "\n",
    "# Loop through the generator until there is no more data or an exception occurs\n",
    "while is_data:\n",
    "    try:\n",
    "        # Get the next batch of data from the generator\n",
    "        t_x, t_y = next(test_generator)\n",
    "        \n",
    "        # Evaluate the model on the batch of data\n",
    "        loss, accuracy = selected_model.evaluate(t_x, t_y)\n",
    "        \n",
    "        # Make predictions on the batch of data\n",
    "        y_pred = selected_model.predict(t_x)\n",
    "        # print(y_pred)\n",
    "        y_pred = y_pred.argmax(axis=1)\n",
    "        # print(f'prediction: {y_pred}, value: {t_y}')\n",
    "        \n",
    "        # Append true labels, predicted labels, loss, and accuracy\n",
    "        y_trues.extend(t_y)\n",
    "        y_preds.extend(y_pred)\n",
    "        losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "    except StopIteration:\n",
    "        # No more data available, exit the loop\n",
    "        is_data = False\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_labels = np.array(y_trues).astype(int)\n",
    "predicted_labels = np.array(y_preds).astype(int)\n",
    "\n",
    "\n",
    "print(f'classe predicted  {np.unique(predicted_labels)}')\n",
    "\n",
    "# Assuming true_labels and predicted_labels are arrays of class labels\n",
    "encoder = OneHotEncoder(sparse=False)  # Set sparse=False for dense output\n",
    "true_labels_onehot = encoder.fit_transform(true_labels.reshape(-1, 1))\n",
    "predicted_labels_onehot = encoder.transform(predicted_labels.reshape(-1, 1))\n",
    "\n",
    "# Now use one-hot encoded labels for OVO ROC AUC\n",
    "auc = roc_auc_score(true_labels_onehot, predicted_labels_onehot, multi_class='ovo')\n",
    "\n",
    "\n",
    "        \n",
    "# Compute mean loss and accuracy\n",
    "mean_loss = np.mean(losses)\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "\n",
    "# Compute other metrics\n",
    "# auc = roc_auc_score(true_labels, predicted_labels,multi_class='ovr')\n",
    "recall = recall_score(true_labels, predicted_labels,average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels,average='macro')\n",
    "\n",
    "# Print or use the computed metrics as needed\n",
    "print(\"Mean Loss:\", mean_loss)\n",
    "print(\"Mean Accuracy:\", mean_accuracy)\n",
    "print(\"AUC:\", auc)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, predicted_labels)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
